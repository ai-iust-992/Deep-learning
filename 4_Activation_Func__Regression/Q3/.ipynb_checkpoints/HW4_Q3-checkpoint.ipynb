{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrFpZXNQB9FW"
   },
   "source": [
    "#### Copyright 2018 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "43_9Kh8LCDPK"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHK6DyunSbs4"
   },
   "source": [
    "# Cat vs. Dog Image Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xvnAdS6YFTQZ"
   },
   "outputs": [],
   "source": [
    "## Set These Parameters\n",
    "last_layer_neurons = 1\n",
    "last_layer_activation = \"sigmoid\"\n",
    "loss_function = \"binary_crossentropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XK-IN_zNgLlT"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhztKtUSFMXp",
    "outputId": "ad79301a-6bf0-4db2-cdd6-163ef3b46c0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-10 20:33:54--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.13.240, 172.217.15.80, 172.217.164.176, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.13.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68606236 (65M) [application/zip]\n",
      "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
      "\n",
      "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   191MB/s    in 0.3s    \n",
      "\n",
      "2021-04-10 20:33:54 (191 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \\\n",
    "   https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip -O \\\n",
    "   /tmp/cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LWkSRoJRfvGL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n",
    "  \n",
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n",
    "train_cat_fnames = os.listdir(train_cats_dir)\n",
    "train_dog_fnames = os.listdir(train_dogs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02r1oXaegECk"
   },
   "source": [
    "Next, let's apply the `datagen` transformations to a cat image from the training set to produce five random variants. Rerun the cell a few times to see fresh batches of random variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8HgwcAbmdcu",
    "outputId": "48a6c515-6d7c-4451-8808-32e27e00cae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Adding rescale, rotation_range, width_shift_range, height_shift_range,\n",
    "# shear_range, zoom_range, and horizontal flip to our ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    # shear_range=0.2,\n",
    "    # zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#\n",
    "class_mode = 'binary' if last_layer_neurons == 1 else 'categorical'\n",
    "\n",
    "# Flow training images in batches of 32 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # This is the source directory for training images\n",
    "        target_size=(150, 150),  # All images will be resized to 150x150\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode=class_mode)\n",
    "\n",
    "# Flow validation images in batches of 32 using val_datagen generator\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode=class_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-3PrfDwDJjB"
   },
   "source": [
    "If we train a new network using this data augmentation configuration, our network will never see the same input twice. However the inputs that it sees are still heavily intercorrelated, so this might not be quite enough to completely get rid of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYguAfH3gyv6"
   },
   "source": [
    "## Building a Small ConvNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SVC4FgxiDje6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from time import time\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\n",
    "# the three color channels: R, G, and B\n",
    "img_input = layers.Input(shape=(150, 150, 3))\n",
    "\n",
    "# First convolution extracts 32 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Conv2D(32, 3, activation='relu')(img_input) \n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Second convolution extracts 64 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x) \n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Third convolution extracts 128 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Convolution2D(128, 3, activation='relu')(x) \n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Flatten feature map to a 1-dim tensor\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Create a fully connected layer with ReLU activation and 512 hidden units\n",
    "x = layers.Dense(512, activation='relu')(x) \n",
    "\n",
    "# Add a dropout rate of 0.5\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# add output layer\n",
    "output = layers.Dense(last_layer_neurons, activation=last_layer_activation)(x)\n",
    "\n",
    "# Configure and compile the model\n",
    "model = Model(img_input, output)\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimizers.RMSprop(lr=1e-3),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdW6geEVi2S8",
    "outputId": "940a1810-d910-4530-f57a-b922b436f450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 [==============================] - 19s 179ms/step - loss: 1.1855 - acc: 0.5248 - val_loss: 0.6903 - val_acc: 0.5290\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 17s 170ms/step - loss: 0.7052 - acc: 0.5826 - val_loss: 0.7325 - val_acc: 0.5690\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 17s 171ms/step - loss: 0.6423 - acc: 0.6348 - val_loss: 0.5968 - val_acc: 0.6720\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 17s 168ms/step - loss: 0.6102 - acc: 0.6827 - val_loss: 0.5690 - val_acc: 0.6940\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 17s 170ms/step - loss: 0.5824 - acc: 0.6965 - val_loss: 0.5662 - val_acc: 0.6930\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 17s 170ms/step - loss: 0.5834 - acc: 0.6964 - val_loss: 0.5794 - val_acc: 0.6910\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 17s 171ms/step - loss: 0.5650 - acc: 0.7248 - val_loss: 0.5236 - val_acc: 0.7370\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 17s 170ms/step - loss: 0.5447 - acc: 0.7396 - val_loss: 0.5042 - val_acc: 0.7410\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 17s 170ms/step - loss: 0.5269 - acc: 0.7317 - val_loss: 0.5233 - val_acc: 0.7390\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 17s 170ms/step - loss: 0.5164 - acc: 0.7722 - val_loss: 0.5908 - val_acc: 0.6790\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 17s 169ms/step - loss: 0.5347 - acc: 0.7438 - val_loss: 0.5130 - val_acc: 0.7540\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 17s 169ms/step - loss: 0.5037 - acc: 0.7635 - val_loss: 0.5036 - val_acc: 0.7670\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 17s 169ms/step - loss: 0.4850 - acc: 0.7604 - val_loss: 0.6352 - val_acc: 0.7320\n"
     ]
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir='logs/number_of_last_layer_neurons{}last_layer_activation_{}_loss_function_{}_{}'.format(last_layer_neurons,last_layer_activation, loss_function, time()))\n",
    "my_callbacks = [\n",
    "      tensorboard,\n",
    "]\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      epochs=50,\n",
    "      validation_data=validation_generator,\n",
    "      verbose=1,\n",
    "      callbacks=my_callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LTWMLV6SUvP"
   },
   "source": [
    "Note that with data augmentation in place, the 2,000 training images are randomly transformed each time a new training epoch runs, which means that the model will never see the same image twice during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZqvC9UJlWc2"
   },
   "source": [
    "## Evaluate the Results\n",
    "\n",
    "Let's evaluate the results of model training with data augmentation and dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKCjHegASXaA"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# evaluate best model\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "\n",
    "\n",
    "\n",
    "# Retrieve a list of accuracy results on training and validation data\n",
    "# sets for each training epoch\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "# Retrieve a list of list results on training and validation data\n",
    "# sets for each training epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Get number of epochs\n",
    "epochs = range(len(acc))\n",
    "\n",
    "# Plot training and validation accuracy per epoch\n",
    "plt.plot(epochs, acc)\n",
    "plt.plot(epochs, val_acc)\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot training and validation loss per epoch\n",
    "plt.plot(epochs, loss)\n",
    "plt.plot(epochs, val_loss)\n",
    "plt.title('Training and validation loss')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GrFpZXNQB9FW"
   ],
   "name": "HW4_Q3 (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
